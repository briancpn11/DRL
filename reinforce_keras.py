# -*- coding: utf-8 -*-
"""reinforce_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1raA72iTgZEHwk4QW7yI1KLF8z5k_wH0G
"""

from keras.layers import Dense, Activation, Input
from keras.models import Model, load_model
from keras.optimizers import Adam
import keras.backend as K
import numpy as np

class Agent(object):
  def __init__(self, alpha, gamma = 0.99, n_actions = 4, layer1_size = 16, layer2_size = 16, 
               input_dims = 128, fname = 'reinforce.h5'):
    self.lr = alpha
    self.gamma = gamma
    self.G = 0
    self.n_actions = n_actions
    self.fc1_dims = layer1_size
    self.fc2_dims = layer2_size
    self.input_dims = input_dims
    self.state_memory = []
    self.action_memory = []
    self.reward_memory = []

    self.policy, self.predict = self.build_policy_network()
    self.action_space = [i for i in range(n_actions)]
    self.model_file = fname
  
  def build_policy_network(self):
    input = Input(shape=(self.input_dims,))
    advantages = Input(shape=[1])
    dense1 = Dense(self.fc1_dims, activation='relu')(input)
    dense2 = Dense(self.fc2_dims, activation='relu')(dense1)
    probs = Dense(self.n_actions, activation='softmax')(dense2)

    def custom_loss(y_true, y_pred):
      out = K.clip(y_pred, 1e-8, 1-1e-8)
      log_lik = y_true * K.log(out)

      return K.sum(-log_lik * advantages)

    policy = Model(input=[input, advantages], output = [probs])
    policy.compile(optimizer = Adam(lr = self.lr), loss = custom_loss)

    predict = Model(input=[input], output=[probs])

    return policy, predict

  def choose_action(self, observation):
    state = observation[np.newaxis, :]
    probabilities = self.predict.predict(state)[0]
    action = np.random.choice(self.action_space, p = probabilities)

    return action

  def store_transition(self, observation, action, reward):
    self.action_memory.append(action)
    self.state_memory.append(observation)
    self.reward_memory.append(reward)

  def learn(self):
    state_memory = np.array(self.state_memory)
    action_memory = np.array(self.action_memory)
    reward_memory = np.array(self.reward_memory)

    actions = np.zeros([len(action_memory), self.n_actions])
    actions[np.arange(len(action_memory)), action_memory] = 1

    G = np.zeros_like(reward_memory)
    for t in range(len(reward_memory)):
      G_sum = 0
      discount = 1
      for k in range(t, len(reward_memory)):
        G_sum += reward_memory[k] * discount
        discount *= self.gamma

      G[t] = G_sum

    mean = np.mean(G)
    std = np.std(G) if np.std(G) > 0 else 1
    self.G = (G - mean) / std

    cost = self.policy.train_on_batch([state_memory, self.G], actions)

    self.state_memory = []
    self.action_memory = []
    self.reward_memory = []

    #return cost

  def save_model(self):
    self.policy.save(self.model_file)

  def load_model(self):
    self.policy = load_model(self.model_file)

#!pip install box2d-py

import gym
import matplotlib.pyplot as plt
import numpy as np

if __name__ == "__main__":
  agent = Agent(alpha = 0.0005, input_dims=8, gamma = 0.99, n_actions = 4, layer1_size=64, layer2_size=64)
  env = gym.make('LunarLander-v2')
  score_history = []

  n_episodes = 2000

  for i in range(n_episodes):
    done = False
    score = 0
    observation = env.reset()
    while not done:
      action = agent.choose_action(observation)
      observation_, reward, done, info = env.step(action)
      agent.store_transition(observation, action, reward)
      observation = observation_
      score += reward

    score_history.append(score)

    agent.learn()

    print('episode ', i, 'score %.1f' % score, 'average_score %.1f' % np.mean(score_history[-100:]))

#    return score_history

#reward_history = run_experiment()
plt.plot(score_history)

